{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Transfer Learning Jasper] Russian Numbers\n\n\nIt is a toy project about ASR model for russian language on small dataset. \n\nAuthors: [@shonenkov](https://www.kaggle.com/shonenkov) & [@artemsolomin](https://www.kaggle.com/artemsolomin)\n\nFor students of [Sirius](https://sochisirius.ru/)"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install torchaudio > /dev/null\n!pip install inflect==4.1.0  > /dev/null\n!pip install toml > /dev/null\n!pip install unidecode==1.1.1 > /dev/null\n!pip install soundfile > /dev/null\n!pip install num2words==0.5.10 > /dev/null\n!git clone https://github.com/NVIDIA/DeepLearningExamples > /dev/null\n\ndef replace_import_apex(path):\n    !cp '{path}' './tmp.py'\n    fin = open('./tmp.py', \"rt\")\n    fout = open(path, \"wt\")\n    for line in fin:\n        fout.write(line.replace('from apex import amp', '#from apex import amp'))\n    fin.close()\n    fout.close()\n    !rm './tmp.py'\n\nreplace_import_apex('./DeepLearningExamples/PyTorch/SpeechRecognition/Jasper/model.py')\nreplace_import_apex('./DeepLearningExamples/PyTorch/SpeechRecognition/Jasper/parts/features.py')","execution_count":2,"outputs":[{"output_type":"stream","text":"Cloning into 'DeepLearningExamples'...\nremote: Enumerating objects: 113, done.\u001b[K\nremote: Counting objects: 100% (113/113), done.\u001b[K\nremote: Compressing objects: 100% (92/92), done.\u001b[K\nremote: Total 8907 (delta 31), reused 71 (delta 18), pack-reused 8794\u001b[K\nReceiving objects: 100% (8907/8907), 45.46 MiB | 18.41 MiB/s, done.\nResolving deltas: 100% (4638/4638), done.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport random\nimport num2words\nimport os\nfrom datetime import datetime\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torchaudio\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Splitting "},{"metadata":{"trusted":true},"cell_type":"code","source":"marking = pd.read_csv('../input/russian-numbers-asr/train.csv')\nmarking['text_number'] = marking['number'].apply(lambda x: num2words.num2words(x, lang='ru'))\n\ndef get_stratify_group(row):\n    stratify_group = row['gender']\n    stratify_group += '_' + str(len(row['text_number'].split()))\n    return stratify_group\n\nmarking['stratify_group'] = marking.apply(get_stratify_group, axis=1)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nmarking.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=marking.index, y=marking['stratify_group'])):\n    marking.loc[marking.iloc[val_index].index, 'fold'] = fold_number\n\nmarking.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  % (min_groups, self.n_splits)), UserWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                   path  gender  number  \\\n0  train/e332b996d3.wav  female  157105   \n1  train/e25afda49a.wav  female  374554   \n2  train/364f147340.wav    male  688694   \n3  train/5e0954b206.wav  female  265381   \n4  train/7130a67690.wav    male  955415   \n\n                                         text_number stratify_group  fold  \n0                  сто пятьдесят семь тысяч сто пять       female_6     3  \n1  триста семьдесят четыре тысячи пятьсот пятьдес...       female_7     1  \n2  шестьсот восемьдесят восемь тысяч шестьсот дев...         male_7     4  \n3  двести шестьдесят пять тысяч триста восемьдеся...       female_7     3  \n4  девятьсот пятьдесят пять тысяч четыреста пятна...         male_6     1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>gender</th>\n      <th>number</th>\n      <th>text_number</th>\n      <th>stratify_group</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train/e332b996d3.wav</td>\n      <td>female</td>\n      <td>157105</td>\n      <td>сто пятьдесят семь тысяч сто пять</td>\n      <td>female_6</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train/e25afda49a.wav</td>\n      <td>female</td>\n      <td>374554</td>\n      <td>триста семьдесят четыре тысячи пятьсот пятьдес...</td>\n      <td>female_7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train/364f147340.wav</td>\n      <td>male</td>\n      <td>688694</td>\n      <td>шестьсот восемьдесят восемь тысяч шестьсот дев...</td>\n      <td>male_7</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train/5e0954b206.wav</td>\n      <td>female</td>\n      <td>265381</td>\n      <td>двести шестьдесят пять тысяч триста восемьдеся...</td>\n      <td>female_7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train/7130a67690.wav</td>\n      <td>male</td>\n      <td>955415</td>\n      <td>девятьсот пятьдесят пять тысяч четыреста пятна...</td>\n      <td>male_6</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Char Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nclass CharLabels(object):\n\n    def __init__(self):\n        self.chars = [\n            ' ', 'а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и',\n            'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у',\n            'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', \n            'я',\n        ]\n        self.index = {c: i for i, c in enumerate(self.chars)}\n        self.re_compile_clean_text = re.compile(r'[^' + ''.join(self.chars[1:])+ r'\\s]')\n\n    def __len__(self):\n        return len(self.chars)\n\n    def __call__(self, sentence):\n        targets = []\n        for c in sentence.strip().lower():\n            targets.append(self.index[c])\n        return targets\n    \n    def postprocess_indexes(self, preds, max_repeat=1):\n        result = []\n        last_value = -1\n        repeated = 0\n        for pred in preds:\n            if pred == last_value:\n                repeated += 1\n            if pred != last_value:\n                repeated = 0\n                last_value = pred\n            if repeated >= max_repeat:\n                continue\n            result.append(pred)\n        return result\n\n    def get_text(self, indexes):\n        indexes = self.postprocess_indexes(indexes)\n        chars = []\n        for index in indexes:\n            if index >= len(self.chars):\n                continue\n            chars.append(self.chars[index])\n        return ''.join(chars).strip()\n\n    def clean_text(self, text):\n        return re.sub(self.re_compile_clean_text, '', text.lower())","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CHAR_LABELS = CharLabels()\n\nCHAR_LABELS.get_text(CHAR_LABELS(marking.iloc[0]['text_number'])), CHAR_LABELS(marking.iloc[0]['text_number'])[:10]","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"('сто пятьдесят семь тысяч сто пять', [19, 20, 16, 0, 17, 33, 20, 30, 5, 6])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Retriever"},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS_COUNT = len(CHAR_LABELS)\nTRAIN_PATH = '../input/russian-numbers-asr/train-sr16k/train-sr16k'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, paths, texts, audio_augs=None):\n        super().__init__()\n        self.paths = paths\n        self.texts = texts\n        self.audio_augs = audio_augs\n\n    def __getitem__(self, idx):\n        path = self.paths[idx].split('/')[-1]\n        waveform, sample_rate = torchaudio.load(f'{TRAIN_PATH}/{path}', normalization=True)\n        \n        ####################\n        # Audio Augs: TODO #\n        if self.audio_augs:\n            waveform = self.audio_augs(waveform)\n        ####################\n\n        text = self.texts[idx]\n\n        return {\n            'waveform': waveform.squeeze(0),\n            'labels': torch.tensor(CHAR_LABELS(text), dtype=torch.int),\n            'text': text,\n            'path': path,\n        }\n\n    def __len__(self) -> int:\n        return self.paths.shape[0]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(waveform):\n    noise = waveform.clone().normal_(0.0, 0.005)\n    return waveform + noise","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\npath = '0007c21c23.wav'\nwaveform, sample_rate = torchaudio.load(f'{TRAIN_PATH}/{path}', normalization=True)\nprint(\"Shape of waveform test: {}\".format(waveform.size()))\nprint(\"Sample rate of waveform test: {}\".format(sample_rate))\n\nplt.figure()\nplt.plot(waveform.t().numpy())","execution_count":9,"outputs":[{"output_type":"stream","text":"Shape of waveform test: torch.Size([1, 48706])\nSample rate of waveform test: 16000\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7fd3eb780f10>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(add_noise(waveform).t().numpy())\ntype(add_noise(waveform)), add_noise(waveform).size() ","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(torch.Tensor, torch.Size([1, 48706]))"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2dd3hUVfrHv28SCC2hhhBq6FUQjCCCCIJUFdeOi7Kurrr2XRVx7T8brn13FWXFXdvaG1ZEqgWFIEV6DRAISQCBUNPO74/cGSaTuXduObfO+3mePJm5c+fec+7ce95z3kpCCDAMwzAMACS53QCGYRjGO7BQYBiGYcKwUGAYhmHCsFBgGIZhwrBQYBiGYcKkuN0ALZo1ayays7PdbgbDMIxvWLp06R4hRIbZ73taKGRnZyM3N9ftZjAMw/gGItpm5fusPmIYhmHCsFBgGIZhwrBQYBiGYcKwUGAYhmHCsFBgGIZhwrBQYBiGYcKwUGAYhmHCsFBgdDNr9W4UlRxzuxkMw9gICwVGF8fKKnDdG0txxSuL3W4KwzA2wkKB0UWlUoxp+74jLreEYRg7YaFggKOlFXh29gaUllcCAMorKrG24KDLrXIWAa7UxzBBhoWCAV6cvwnPz9mItxdvBwA8PXsDxjz/Hd74yVKqEV9AILebwDCMA7BQMMDR0goAwAMzVwMAVuzYDwC475NVqKwM7gw6sm9c0pthgg0LBZMcOFKG8ojB8unZ611sjX3MXLELHf72JfL2Ho6772+HSzF94WYIlhwM41tYKBjgle+3hl8vz9+PvYeOh99/tqLAjSbZzpcrq/r1pg4V2V0frsRjX65D7rbf7G4WwzA2wULBJN+uKQy0yXXn/qMor6gMv3/r5yo7SrmKmmz3gWP4Zk0hAKCsvDLmPgzDeB9PF9nxMtHGZbddNYUQmLliF8b0ykLtFGuyvqjkGAZNnat6nljc8f4KS+dkGMYb8ErBYZbk7bPFKD1rdSFufWc5/jFno+Vj7TtcqvqZWstLI1YVQV5BMUzQYaGgkx837bF8jPnri3DxS4vw6g9b4+9skANHqwZyGWkotu9VX/WwDZlhgg0LBZ28tijP8jHmrisCAGwuju/JY5Q1u+QF0V37xlLD3/FSFEPBgaNuN4FhfAsLBZ3MWl1o+RivL1L34Fm8dR9eX5QXjoUwymvKsfcdLjP1fatELiAOHy93pQ0A8PGyfAx8fC4Wb93nWhsYxs+wUHCFmjqYS15ehPs/XY0Lp/1o6cjfrrUuvKxiZqUhi9y8KnfYJXn+EwpLt+0Lp1BhGLdgoeABSo6dmN2v8Wkupbw98lViZgi5zj45y1/BhOt3l+DCaYvw2Jdr3W4Kk+CwUPAAD322xu0mGGJOjNVIUcnxGHsyenkvdwcA4LuNxS63hEl0WCi4zNHSCnywNL/aNq+rEO78YGXcfZZu85/6xi0Wb92HGUq0vB1OCAxjBBYKLnCs7MSgH8s9NZRwTy+rdx2w3KYQh3QYibXiGEIcOOqOwTuSVTvlXRc7KTxY3Y14m448UwxjFywUXCAU6LX30PGYuu9l243lDirYX31QOVJq3vtHVooKL8QzeMHoboYLpy1yuwlMAsNCwUXmKHEL0VgdUK0ETJOkgIONRYfkHMgCXhBMZthzyLx9JnvKF8ie8oXE1jCJBgsFF1iZv1/zc6PVzcoqqs/ud3igZObUr9a53YSEI3KF+K+5GwNd44OxDxYKEvmf4g4Zjx37qiJu1SblRme4D35W3Qbx9DcbjB0goCTykPjUNxvw6YqdHN3NGEaKUCCi0US0nog2EdGUGJ//nohWKn8/ElEfGef1Gi8t2CzlOEZVL4UHq6sb/KpLV2PPoeMoq6hE/m/aK6Avf7W/psX4F37Ap8t3Sj1mLOH1mw5jfjz+8u4KDHw8drZbhlHDslAgomQALwAYA6AHgAlE1CNqt60AzhRC9AbwMIDpVs/rN/yylDdTi1ktnbYMNdZl0xch55Fv0eXerzD4iXnYf0R9sJyrYqORxaaiQ1ixYz9ufWe5oe+VHCvD16vUBdbKHTXViVv2GLfJqP120xfKmawwiYGMlUJ/AJuEEFuEEKUA3gEwPnIHIcSPQoiQS81PAFpLOK/n0PL6+T5GllWSZdV1mZcXbom5fdbq3ZaP/dOWqniHkNzRcplNir6cEi3N5RWVGPHMAlPf/et7K3D9m7+gx/1fY8b3W2u4oEZW9Athpulqtqj//JBn/GAB44V5m5A95Qvc/h7X/YiHDKHQCsCOiPf5yjY1rgbwldqHRHQtEeUSUW5xsb+iO/ccUp/Fllcac/X86Jf8+DvZwDQTKjC7Z+h6SY6SCjv3W08jHsJKnYrQiulIaQUe/nyNrtxQMteVMmRjZaXwrX3iaGlF2PX7Q5eeKz8hQyjEmu7GvA2JaBiqhMJdagcTQkwXQuQIIXIyMjIkNM+ffLXK2izbbOCWXruImspIFm/9vM2wa2V0k2QOAJsN5nbK/+0INim2odIo77ASmwL71NRHRr3ZgKrf95KXF+G/SnDlC/M2YeDjc30ZWPfWz/HrizMnkCEU8gG0iXjfGsCu6J2IqDeAVwCMF0LslXBeXxHrgX1jUZ7q/lbH3Oe+tdcDaXqEymizDTEJM74zXojITjllVNE3+Il5YXXTgSPVhcBxm9KYRAufEGaiy1fmH8Dirfvw4GdrsHXPYTw9u+p+Kjggb/XlFNH3Ra4PM+g6iQyhsARAZyJqT0S1AVwGYGbkDkTUFsBHAK4QQrC/pMKKfH+kYYhFpKfPXgmeMkEl55HZNa5P5CB9rCx2/YwjJupq3PPxrzG3R6ZV0Ut5hGPEsKfmG/6+DDYVHUL2lC+w1WIG3mjT3UUvccS4FpaFghCiHMBNAGYBWAvgPSHEaiK6noiuV3a7H0BTAC8S0XIiyrV63uBjbdrrhWheK4Z0tRmpVjZWM2oSu9GyMwHqsS03vvWL4XPFcmYwi9pPd9n0n6SdIx6vfFe1Gn3qG/lp0L9etRvlKiurREdKnIIQ4kshRBchREchxKPKtpeEEC8pr68RQjQWQpys/OXIOG+Q8cKgbhWzdoel2/bhqMoM+oY3jQ+WMvh8pbwYiMjxtkLFVVlPYkKt49qJ2upGNu8sqfJf+cLitY/lBXf9m0ulxRUFDY5odgofeJ8aSdltp8xatl09DcjBY+r68VDVNVn8a+5G5O05jKKDcvXoJREDvkyvZJkuznfpSI+ul137jxou0WrWUWL73iM1YoKWqNwXO/f705vKblgoeBSrg66Z71camNnr2dXsIPXIF+rVx7TauEVi9bfikuN46psNmDjjZwz++7wan7vlMuwEc9YWSktouGrnAZw+dS56PjALGwtLdH/vmdnGTY+bikow5Ml5eM6C+zDDQiGw2B07UFQSf/Zsh9uqU2q1kH3iWFllzBXU85IGHqdUMUb051e/ps/kN299EabNV1fBCCFwzj+/D79fFiNyWw0z928oLiUypkTrPg2CitYOWCi4RDw3QbvjAGJhZGIfnW8pFrIGzkjscudUJ/bvIOvneUpi8kKtn+9xiVlrxzz/HQDgqv8swRNfr8Pm4tiriry9zmbrjbWKfORz9VVnyGbBVIeFgkNEP7DL48ya9KZKMqqr1UK2HCo5ZrxtspPNmSUUV+L10qiRaAn1JRJ987fuOYxnI9Q7w59eoCvQcLJOO8UEkx5O/4wxCeHFgHFYKPic+z5Z5cljmcVosjm7CA2wagFhXnR/1WKl5JiYWKtAq/EEIRZtMRfbumrnwRrbfODf4TlYKDjEqzYlJdulkY/GaFnO95cG13hqlNBg4pPktrqw236xImr1K3N1oosYEiBePXFZgixIsFBwiIUbjCX3268zNcFujbQDD2voU2Ug2+5hR/oBqwFKauojPxop1WIiZHHbu9VXebHURVr3qx3EC+g7N8IQzlTBQsEljseZtUXPutTQMuYVa0T/ykAtZbZZtttQRlRrHPx85S78sr2mD3vJsbJwMjs1rAoFrXgLs8SLnvYCZvIwAcD89fG9kcyoiswECQYdFgouMflDecFBatjtwSS7DnOSDfUlvlmjnm32pv8twwUv/lhj+8QZi3GphXQOetQ0364pND1A+hmjKeRD5OlQ85i9fbaoeE8lKiwUXGL/EfsHBL9pOOyoOfSuCbdDPas0rSA6NeN0NC/M26T5+d5D9q703KC8wr67MjLxnxG1od0rar/BQiHAlDtgJfV6mVG7Fktax9UX7R3/2n2yvEYGekt44Zcya3w22vaPlul3bQ5KBURZsFAIMBUml+pGuPq1JdKOpffhLDMwCzTqOnrvJ7HTTxs5rl61XTx5unir3LIjZmpUyEYrhYlM4tnsIrn1nWU2tsR/sFAIMCt32F+vYd56eSVT9c7X5qwt1H3MrcWxddE7VIzab/4UO5V1NFqePHpkQt6eI3j1B+1BetZq/f3Uw7M2F14C9LlBey1ltR8LB9kJC4UAU2LAs2L/Efc9V/Su4o1orHapPPBnRCS5q6gUuPzfP+H7jfrrEWh5+uhpnh0pQLzAxsIqo+2Pm9WvpZm+7zaYqVagqmYCYxwWCgwAYOKMn205rhFjqVqNYZl8unxnjQyn+4+U4sfNe3HT28ZqNai5lRrJNhsPv+X8D1Xk+7/P1qju88+52gb2WLy8wLj78/VvLtW9r+z06H6GhYJkrrBpcDXL9r1HsLGwRDNoaFNRScwUATK410DqDL0rBbNj7oINxbj1neX463srqh9P+W/UI2y1yjWTadyW7fZrN7JjV8xi1KPoylcX29QS/5HidgOCxncGVBDxqKgUSE6yNnse8uQJNUne1HEx9xnxzEJL59DCSGoFvT01m65hksqDb3YQj2Vs3lhY4kgMSpAQQmDW6kKM6N5cWloRo6uRdbv113oIOrxS8DD3fyo3Qd3dH610fJls5CHX6310+/sr4u8UQbyo1VMf/dbQ8UJcOaOmkJn84UrNynFm0JOB1GsYcfOcvaYQ17+5FC/M24zXF+XZ1qZ4eCVDr9uwUPAwizbLdUl8e/EO9H9sjiGXTqssMJDzyS538Xs+1udmapRYcSB+zIkkm8PHy7G2QL86cq+StK7gwFGptbCN4pUMvW7D6iMdWM2P8qvktMVW6XzPV6qqJD/xxk/bkN20HsoqKlGmESn76fJdOLd3SwdbltgYiUUor6jEjO9PuOYWaGT9ZZyBhYIObnzLmFdKNOf+y1wmRpleLNF8tmIXzu3jrYHS6ELBSP2Ha17XV2LSKm4tFPTmUdq29zDaNa1fbduzszdgQ2EJpk08RUpbjHicvZebXy35oFb+qzW7DqJHy3RLbWPik7Dqo01Fh7B6l74ZvBEViEzi6eOXbquZ4VMvjyqzuePlztQIDipeCcRap1Ndc+aT82tse37ORnwl0ae/UIfnz9HSCpRVVFZLWPjOkh2aQkEt2G/HviPIecScXYipScKuFEY8swCAukdOCDdz+2ilUigtr8SF02pm+NTL7oPHcPBYGX73wg+mjyGTo6UV2Llfv+rgxzh58p2itKISKckRcyuXjApG8lx9vWo3shrWQZ82japtLyo5hiPHrU8S9CQU7H7/1+jWIg3pdWtV2651D6hd2veX5mNPAJMHukXCCgW9zFrtXlTkjn2xH5CjpRXofv/Xlo/f+8FvLB9DFn/4z2L8vFV/srTLX/FGPIhX8gG2bFRX976hoK43ru6PKyI8qPo/Okd6u7RYt7vEkHOB30qg+pWEUR/9/et1yJ7yheFC9/s8kP4hmiAWBjEiELxEdA4ku4atwjiuxGZqZ7zigQR5RpqdrzJJYpcvuSSMUHhxflW6gOFPL8CGQv2BKvd8bDxWYKOB45vBb5l+vaJ3twOn1IsDHtOexZtphVu2MrMsVkm7vVWj+iBjnIQRCiF2HzyGkc+eiOA9dLxceu3aRVvkxhdEUl5Ridlr5GbPtJtO93zldhNsoyJqlurGpHXn/qMY/vQC50/sArFsB4dsKG2ayCScUIim1wOzcN0b+hNn6WHXfvuihqfN34y7P7InGIsxztHS6obZX3faF5NyQCU30ycGCsr4nVheRqw8kkvCCwUA+NZAfn492JnZcpsNxe2dQK1+gd+JTMFtNzILGgUJNinIJSGEwjuL9RVO8SLz1hWFXxccOIoPluZr7O1dgjybNZugzygr8mO7egb52urBYs5IJoqEEApTdKhb3s81XuDdCa7675KwMXPg43Ndbo15ZGaP9RpOGWzLKgR+U/IEHSurCNvCNkZEBCciXGNZLgkhFPRw5wcrsWBDsTRvkuwpX5hyE4zF2H98J+U4brI4bx/++8PWsGH/7o9WYnNxMAazW95ehskfrHAkm2nfh2cDALrd9zVueEuuLcwvzFtfVM2jTdZzFs2mokMoLT9xntCKsLJS4IuVBThWVoGvfi2AEAKPfL4GU79ah2NlFTh0vFxX0sk9h46jtLyyWq2T4+UV2HfYXTd4suuCyiAnJ0fk5hrPWbNwQzHmritCl8w0/M1Chsz+7ZtgsUX/+XUPj0a3+6wHmgWZf0zoi1ve5uLperl8QFv872f/qkRl8t51A3HnByuwzUG31CSyHrTYPStdM5Ns2yb1sHDyMFPHJqKlQogcs22TIhSIaDSA5wEkA3hFCDE16nNSPh8L4AiAPwgh4maZMysU/Jh/nmEYJhKzmYytCgXL6iMiSgbwAoAxAHoAmEBEPaJ2GwOgs/J3LYBpVs/LMAzDyEeGTaE/gE1CiC1CiFIA7wAYH7XPeACviyp+AtCIiLIknJthGIaRiAyh0ApApOtOvrLN6D4AACK6lohyiSi3uNhfYfgMwzB+R4ZQiOUPFm2o0LNP1UYhpgshcoQQORkZGZYbxzAMw+hHhlDIB9Am4n1rALtM7MMwDMO4jIx6CksAdCai9gB2ArgMwOVR+8wEcBMRvQNgAIADQgjbKnTnTR2HikqBkmNlWFtQggn//knX97669Qx0yUzDmz9twwMzV1tuR2pKEo6XW88QOvf2M7G5+DD+5FBJSbvISEtFcclxPHx+L3TMqI+BHZpi1HMLsaHQ//EKmx4dg2nzN+Pp2RtsP9fE09riwXN7oscDs6r50ScKVw5sh4fO64k5a4uwfd8RfPFrgaUqhNGEvH6OlVXgtMfnoF3T+vj3Fadgxg9b8fKCLQCAL24ZjE1FhzD+5Fb4bmMxrpixGCe3aYS7x3RDnzaNkP/bEYx45kTizVm3DcH+I6Von1EfzdPqhI//8OdrMLBjU2zfdwRJRBjRPRN7Dx1H//ZNpPXHKLJcUscCeA5VLqmvCiEeJaLrAUAI8ZLikvovAKNR5ZJ6lRAi7ghn1iU1mg+X5uP291do7vPEhSfh0lPbht8XHjyGWslJ6KcECxllcKdmePOaAVi/uwSjnlsY/wsahG7SJ2etwwvz7MurZCf3ndMDzdNScfPby7D03hFo2iA1/JnfXYjvGdsdfxrSAYD9ffnlvrPRpH5tAFXJ+EorKnHVfxbjl+3xq50Fhc2PjUVyVG4LWdd93cOjUadWsurnFZUCQojq1fYAfPlrAQa0b1Ltvj5eXoEkItRKdjZG2KpLqpTKa0KILwF8GbXtpYjXAsCNMs5lhtG9WsQVCpECAQAy0+tYOufrf+wPAOjaIs3ScSK5cVgn3wqFq07PRlIS4dw+LWt89sC5PfDRLzttzTBqJyGBYDfTft8vLBAAoG7tZNRFMsaelJVQQiFaIMhESyCcOHfN8489qaYzZWqK9rG8SkKkuaif6nzV0SRJN27kIFqvdgq+vu0MKcd1Gq3rcdWg9vjs5sHITE9V3YcBxsQYeADg6sHtkZGW2NeudWP95UgZbRJCKADA8vvPNvW9lyb2k9wSY0y94KRq77u1SHepJfbTJVPeqsopBkTpfhu4MAEhIlx0SmvHz+sG16msyponuFCUScIIhUb1aqt+Frkkj6Z143qWz/3C5eYFixurHLdI8mG2y2ZRg9H5fWuqx5ygb5tGrpzXaU5q3TDmds6UKo+EEQoAUCuZ0CMrHesfGa37OzKW5XVr23OZbxvR2ZbjyqZZA33X0I958aOb3EbCJMIMI3u2cOW8TqOWiM6P945XSSihsPHRsfjy1jNqGIDO6tZc9TuZ6XVwscWl+ekdm1n6vt8Z0EGfe52dK4XHfndS/J0kcM0ZHXBGZ3/83h/++XRcmtMm/o4eYmSPzJjbHz6/l8MtCS4JJRRiMXl017gDxmX9jT04n900uNr7eB4NRvnr2V1w1+huUo9pJ3qHejtVAAUHjtp27EiSk0hzkmGWPw5qL/2Yp7RrjCcu6i39uEZIq2NMPar2LLm1QgsiCS8UbhjaCbVT5F6Gds3svUFvGd4Zfx7aEbv2OzPQOYWdKoCFDlVHAxCuiCaT8SfLs1W0alQXF/SNmXrMcRbeOQy3DLeuBvWjPcqrJKxQWHDnUHzzlyG2HNupG7RXq9hGN6+hdwVg9rqtf2Q0nowz43XyWlXaULhKlsvpVYOy8cOUs/DMpSfH/Pyh83pKOY8eRnTPRGMNJw8jJCXsSCafhL2U7ZrWt80F0imjV6tG/vDN1ns5zDzYf7+oN1JTknFxHN34I+f3wt/Gyle5dcxoUGNbi4Zyf5cfppyFlpJ+6wfOVR/0v/nLEIzp5ZzB+qmLqwR5j6wTz+Hk0V1V9x+uoZbjlYI8ElYo2AnpHgaZSIzaFFqk18ElOg2lRIRrh3Q00yxNYqk+zu0tt1RIC4vR9XrpkpnmmGvnRzecHnYTD63iWjasg+5Z6nE4d2oIDH7i5MFCwQZkPVdpcWIUZGgpbj+7i/WDxCFF59LJyGzv1wdHYv6dQ022SB6xUi7IHlhlHe3OUbEH1b5tT8Q42JlCIpJ+bRuHX6fXrQUAGN0rC8ka165tE3VbHccpyIOFgod5/EJtryirMuGCvq1wswQjXzz0pvwwMh6l1akl3avLq8ga7xasj21s//D607HlsbEAoDko20V6nVpYeu8I3DOuu6ZQqldbfZLEIkEeLBR0YeyWc+q5atrAmpFOzdgom54t9aXmuHFYJ137qfmqm6Vny3TcMdL+FZMZ+kiMVN5/tDTm9qQkCgtukjAiTPu98Qj+pg1SkZxEpm0DvFCQBwsFG3AqO2LHZjWNnF7kD6dn69qvXVN9rrxndjVfkS/33hHV1CUA8MUt3k0y+OmNg6SpRsoq4q8tZawU4nkUjdaIvjarvmL1kTxYKEhGK4+SUeIZrBvWqyXlPFkN7TVk6n1g9RroJ0SlOTdCswap6BThMdSpedXr/u2bGj7WzWfpW9l4hUkD28XdR4YXT7wjaMUFOVx6gIkB/wQeRli2GqhzToSHzCntGmvs6T2sjlsPKL74TevXxnvXDQQAU5Wu9OZ08gpaSSFDyPD3j2dD0orj4Bm/+yROCk6HkFHJzgkejUjt4RUfb73NMDpw5N47otr7BqkpWPfwaKQkUY0KWkbwy28dQs8kQ4b6KN4RtFrhlXsxkeGVgoexM94h0t016LnoY83o69RKriEQ9No+QlgRCe2b1bfwbXPokWFS1EcWpIIb3k9MdVgo6MDIfSpz7miX+qhbi7RqS/w7VPzXncbt4eCGoTWD22bdpp4KxUqKo5BvvpPoEgpS4hS0jzFJQ/hyugr34Z9ABz7TEsTl+jOrD352+vv7KTVztOfLsK4ZmjW2raiP3FA9OXHGoV0z4sabaNlvnAqeY9RhoSCZC/p6vyzi+Q5myDRyLreNjE2j1EwzJp2qub+VcT1oE41IrPyOrD5yHxYKOjByn947rru889qgUHn0d1yMRIsJEbUzrHjRxOOJC52vY+DE6oQA9NIZrBgLOeorxgosFCQj86a2w6ZwYT9nVzJGBKoXhoNhXfUXyLHy62SmO2/cN1rQxixWPLp4peA+7JKaYCRKvqAQF/Qzpiob2bMFRvdsgQ4Z8b2D/KYCGuWDOs7skuo+LBQ8jAz1Ud+2jbBs+34JrbEfO8aDIZ2Np8R46YpTdO1ntZhOWmoKSo6XWzqGEZyw2YTO0TwtFUUlxw1/n72P3Id/goBzQd9WmNC/Ki3EbJsqzWnB8z51XpyonTjuVgcy2NrF6R2Npw0B1L2P2CvJOXilEHCyGtbF5QPa4e6x3ZBex3nfeLexc3I81EJiPgBonqadc2rMSd5X90QTutyZJvNpqdkUZt40yGSLGKOwUAg4I5Q0034QCHaoN+zU+/dsab7us0B8RwK/2SwiOUmlJnZ0htpo1Bw1GjoQ7HfjMPmV+fwIq488jN5U0onEtUM6uN0EaaT5QFAb5aHxVckGzRqM3fQ+unOU/BrefoRXCh6ml8psy0/Inv3XSvZmwSOj+C3Dql5aN66ayJi97Ox95D68UtCBn5fxQYN/C38wrJv+eI9InIqlYNRhocAAAEZ0l1viMoTsiR/PJP2BWjzMqdnadSvUbApup0BJJFgo6CAR7schXZrZclzZl87ob2EmTsErBHFVNNkjGXkZdSwJBSJqQkSziWij8r9GCS8iakNE84hoLRGtJqJbrZzTDYL4cEZj10ysXm256gCj7YxXL5hxFispMOyEPY9OYPUXmgJgjhCiM4A5yvtoygHcLoToDuA0ADcSUQ+L52Ukc6ZNM+oeFpKjxaKJpLrUDBNJ/VS2ZYSwKhTGA3hNef0agPOjdxBCFAghflFelwBYC8C53M0SkDGJNhvh6RRtfeL+esXAbLebIJXuWepCs3ZKAugtbeTjG053uwm+xKpQyBRCFABVgz8ATZcDIsoG0BfAzxr7XEtEuUSUW1xcbLF5cpChPsoIeMlLp/BKugOjpTvVaJCqnqCwU3P1Aj9+4KZhnVw9f5dM/dcvEVTEeom7ZiKibwHEire/x8iJiKgBgA8B3CaEOKi2nxBiOoDpAJCTkxOYn8qNoexONup5HjvrcHsNK1HJRq/SV7eeYfpciU5coSCEGKH2GREVElGWEKKAiLIAFKnsVwtVAuEtIcRHplvrY9xwqUu0iOi6tZJxtKzC7WboQggReDfL6DQe8+8Y6ti5Od7BPFbVRzMBTFJeTwLwafQOVHXnzwCwVgjxjMXz+RY3Hv9EmoUCwPOXnex2E3STiOoKL3uCnd3DnjgdP2JVKEwFcEAX3ToAABIfSURBVDYRbQRwtvIeRNSSiL5U9hkE4AoAZxHRcuVvrMXzMgp/GdHF7SZ4hlQfFRByepFw9eD2zp4witEWC/zYfb2M2B+CjqU1lhBiL4DhMbbvAjBWef09OK2+bVcgtZY3/b6DjtV6x2HVkcp9MVLyzNWN8p+RTJJkmGfsh0cUh0g0VY5Z8qaOQ7cWsWdt/5jQ1+HWBAcjMuyu0XKyhQ7qdCJK3svmkzo8saoGXw2HsOuhkKmbfuriPvIOZgE1L5VxJ2U53JLEpHdrc9l5u0apYE7vaE/qFNksvHOY203wFCwUHMLDE6UwVlUislAToF6JUQCATrJ00A5dciOnqSUxFUX/9toJ8PRi50q7ebq5KnFBhYWCQ3h5+ew1vKZq6x8js+fEAW2lHDu9bmyzXorBuhHxMCLvT82ukcLMNBNPawcA6NS8gaXj1NMI8lNDT0bdLJNlQ4MMCwWHMDrQjerpvIucmUnrhP5tpLejp+R8SVZJivGUyIoxeOx3J8Xc/tB5vaQcP0S80p+RmO1bs7SaLqfn9WmJvKnjLBcVMlNOtm7t+IKkY4Y1YRVEWCjoIEWC2iL6ORvSRTsB3bCu+oqUaD3sTqxO1PLmW+GuMf4vi9iqUV1d+zVUSfDnx7Qo/5zQT8pxZOYJS+cgNsOwUNCBXsPbBf3U8/xFD9AdmtXXPJYb2v2zTFTL0kroZpZayUmeUreZKUDkpfYD9gTLDe5U3ZDcRFJw2qWnyl99MvphoaAD3ctpAw+eV4y6kRhd4o8/uSVG9bAWlKTGd5O94xFiJvBL78/rZqF6K+RNHYc3rxlgizunzEcj3rOrVuktkeG1lWME7+brqhJPIINQAXgvED2wNGsgL12DV4vO6GX2X87Eu0t2YHh3czWZ3aaLRQN4EGGh4BDRExZZhko3FxxeW+zUdmiAPaWdPO8cp4iOIZBFmyb1cIfkbLxGjOJWCYL9Sjb+nqb4CHcS4iUWp3Wo7jo67fdyDJ9miOWx5CYjOOFbTGTGZAQFviIy0RiF7VIda3lqyDL8uYXR1N/Rqy+rvvFq1NdRd/rmYZ1tObcXuP5MrmccZFgoyERj1WtXQFbftuqqjAEdvF0CNB4vXH5ipn/HSO9kg33gvJ5x97nk1DbImzrOgdY4T7TXkWy8ppZMNFgoOITRlYJds1wnGdTJmlCKDGK76SzvzLytVBADgFvOcrdMpR786N/vU0cuz8FCwSGM3q+nxkit4DV+11c9LgMAnr7YWtEbIkKD1BSkpui/Tf9+Ue+I71s6vWNEqwD7tW1k6jgPnttDRnMAaMcK2H1dzao9YzXLL/eAl2Ch4BBBLL3YMk7Urowu/3Lf2fj1wVG6929jkyvrAyYHXD3F66Ojnz+6YZCpc2XHCYg0gpYKx271zplxov3ViPWMaRX3edFFRwQvw0KBsQ0Zg0ftlCTUNrBSiKRBqjU1TyRjTabtvmNUVzxzSfWU5N1siAIPEmYnUJHfCqUJeeR89RxSenIjJSL+UxwyjAaRPu4tJGbAzEyvg3vHdbdcy3dE90zTAiYeiW6fjZQlqSlVA77fgwPdgK+YQwRQe5RwXHNGB7Rrak1F88qknBrbvHhvaMU1eLG9gHEPvyb1/O2ybRcsFBwi+oa96JTWLrXEObw6eDhNvThqikinAqsF7mPRwkQRmdP86M5s8H7r08acQT/osFBwiOgBslcrcyUP/QT7m1cxMk7SwMgJQq9W5u0NamPiQImpqL3Ma1f1d7sJgYCFgkMYmcRcEMfV007uHdfdtXPLIJT/aICkMpAyiJeJk4gwaWA7pNdJwe8HtDN9niDJ4MjV1UsT9XkJJYrwsxsWCg4RuVK4dbh2IFYzFwuspOosmpPj0aRwp7RrjLtGd8O0iae43RRDPDS+F1Y+OAqNfZ6aRBZPXHgi3mSozoJTjBxYKDhEpJtddjPvpIWugU6dzyCbUx2YhYjw56EdPZf36emL++ie8TLAuX1ahl9bre4Xq8ohxyiowy6pEtEaTp22ud4SZzVilZCMq2+ioHoicqGLjgWdM+WmTGkqsZ6EE8R69uxyCw4CLBQCSrxyn1YJLSjYD9w79FNJjnj9ELlZTbu18G7w3XVndsDwbpwm3AosFCSiuRpg90zGZtQS9fm15GTbJvVwks766CHuHlPTUaJlI3lBjIkAT/McwkhgjR8zVDLBY6TLhXkWTh5WLX26We4/J36qc+YELBQcwkgg17UGl/sXS9RX63Vr5MC04DP9yprR136kbu1kfH7z4PD7C/q55/LtB1goOESsMXSESrFzowngrOb3NwMHpiUm8dypvUpksOgzl1hL6R50WChIRNP7KIZUMJv90054AeBvJvRva+vxvXjP6qV/dpNwcCOjDiuvA4BMVQ6rj4LFJTnBz7Gll/euH+h2E3wBCwWHqKej2LtZWJXDhIhMDxHUGtGMvfBayiGuHty+xjY7B/MGqeaEkMxI4CS+uxznr2d3QVqdFLx8hb/SfDDewdJjS0RNiGg2EW1U/qsmxCGiZCJaRkSfWzmnl9HSqESG6hvN+26G4SpG7HiMkxjp2TyN/cOdpn5qCn59cBRG2ZCCm0kMrM7lpgCYI4ToDGCO8l6NWwGstXi+QBFLL39GZzk5hUyXNGRjAcMkNFaFwngArymvXwNwfqydiKg1gHEAXrF4Pk+jVxskNPa0mvyLYRjGClaFQqYQogAAlP9qOovnAEwGUBnvgER0LRHlElFucXGxxeb5D56nM16G1VLBJ65QIKJviWhVjL/xek5AROcAKBJCLNWzvxBiuhAiRwiRk5FRM+VtkBjQ3r9FQXxZrpGxxMgemejUXG7GVcZ7xHVREUKMUPuMiAqJKEsIUUBEWQCKYuw2CMB5RDQWQB0A6UT0phBioulW+5yQofnKge3wwMzVLrfGHCwUGCaYWFUfzQQwSXk9CcCn0TsIIe4WQrQWQmQDuAzA3EQWCJHEMureNaab4eO0aFjdy+eqQdlmmwQAuOzUNpa+zwSTZJ9mW2WMYVUoTAVwNhFtBHC28h5E1JKIvrTaOL+h95FRMzS3bFgHHTOML8+vGlQ9BkItr75eMiSUA5VxDMZ9aiVX3dW1U5Lwf+N7udwaxgkshdkKIfYCGB5j+y4AY2Nsnw9gvpVzusV1Qzrg5YVbNPexGotm1h00egbnhQDnT24c5HYTGAnMvGkw5q4rwo3DOrndFMYhOM2FCzgRvGYFGa1r5rOSjUxsumelo3uWdyutMfLhRAR6cWAcz2ooJwLYalO9sNJgGMYdWCg4SNfMNM3Pg1LUhGEY/8JCwUG6ZVUJhUhD8yPnnzDeyUxG5zact55h/Ak/uS4z8bR26JhRH38cVDOLqlnqp7qfKoNzKDGMP2GhoBM9g3bHjPq6jhVtaJ5z+1Dcf24PU+0KHzPikMO6msuQGuKSHGNxCvVruy+EGIaRAwsFnWSmxzcC/3moe25710TUa7A6S2/TpB4u6Ku/uPmnNw2u9v62Ef6s48swDAsFqbgZ8Xn7yK5Sj2fEA6lNk7rV3vu1uDvDMCwUAkMo5bZeFZZMvB53wTCMfjh4LUBsfHQMktjAyzCMBVgoBIhaDrmBXnFaO83P2fOIYfwLq4+YmHRo5rwaimEY92GhwMTkBk6AxjAJCQsFBxncqRkAoHOm96tXaXlStWpcV/UzhmH8DdsUHOTinDYY3j3T9+ks/nRGB7ebwDCMTfBKwWH8JBD6tW0UcztX4GKY4MJCgVGFU2gzTOLBQsEA9TRy/DSqV8vBlniLSA/UAe2buNcQhmEsw0LBAL1bN1T97MwuGQ62xLu8e91At5vAMIwFWCgwDMMwYVgoSEIksAI+mSOYGSYwsFAwwJUDs91ugqNcPVhf4Z+kJMLXt52BZy/tY3OLGIaxG45TMMDYk7LcboKjnNO7Jc7p3RLz1hWhuOQ4Jn+4UtXY3q1FOrq1SHe4hQzDyIaFAhOXYd2ao7yiEqt3HcCNnP6CYQINCwVJ3CG5yI3XSElOwkPje7ndDIZhbIZtChIY3q052jat53YzGIZhLMNCgWEYhgnDQkECN53FenaGYYIBCwUJ9G3b2O0mMAzDSIGFAsMwDBOGhQLDMAwThoUCwzAME4aFgkEW3Dm02vv+2ZwqmmGY4GBJKBBREyKaTUQblf8xLa5E1IiIPiCidUS0loh8m1+5dWOOR2AYJrhYXSlMATBHCNEZwBzlfSyeB/C1EKIbgD4A1lo8r2twKUqGYYKMVaEwHsBryuvXAJwfvQMRpQMYAmAGAAghSoUQ+y2e11Vm/2VI+PXD53PqB4ZhgoNVoZAphCgAAOV/8xj7dABQDOA/RLSMiF4hovpqBySia4kol4hyi4uLLTbPHtLqnCi92bVFmostYRiGkUtcoUBE3xLRqhh/43WeIwVAPwDThBB9ARyGupoJQojpQogcIURORoY3S1y2aFjH7SYwDMPYQtwsqUKIEWqfEVEhEWUJIQqIKAtAUYzd8gHkCyF+Vt5/AA2hwDAMw7iH1dTZMwFMAjBV+f9p9A5CiN1EtIOIugoh1gMYDmCNxfO6zsc3nI61BSVuN4NhGEYqVoXCVADvEdHVALYDuBgAiKglgFeEEGOV/W4G8BYR1QawBcBVFs/rOn3bNuacRwzDBA5LQkEIsRdVM//o7bsAjI14vxxAjpVzMQzDMPbDEc0MwzBMGBYKDMMwTBgWCgzDMEwYFgoMwzBMGBYKDMMwTBgWCgzDMEwYFgoMwzBMGBJCuN0GVYioGMA2k19vBmCPxOb4hUTtN5C4fU/UfgOJ23etfrcTQphOHOdpoWAFIsoVQiRcwFyi9htI3L4nar+BxO27nf1m9RHDMAwThoUCwzAMEybIQmG62w1wiUTtN5C4fU/UfgOJ23fb+h1YmwLDMAxjnCCvFBiGYRiDsFBgGIZhwgROKBDRaCJaT0SbiMiXZT+J6FUiKiKiVRHbmhDRbCLaqPxvHPHZ3Up/1xPRqIjtpxDRr8pn/yAiUranEtG7yvafiSjbyf5pQURtiGgeEa0lotVEdKuyPdD9J6I6RLSYiFYo/X5I2R7ofocgomQiWkZEnyvvE6XfeUqblxNRrrLN3b4LIQLzByAZwGYAHQDUBrACQA+322WiH0MA9AOwKmLb3wFMUV5PAfCE8rqH0s9UAO2V/icrny0GMBAAAfgKwBhl+w0AXlJeXwbgXbf7HNHPLAD9lNdpADYofQx0/5U2NlBe1wLwM4DTgt7viP7/FcD/AHyeYPd7HoBmUdtc7bvrF0XyBR4IYFbE+7sB3O12u0z2JRvVhcJ6AFnK6ywA62P1EcAs5TpkAVgXsX0CgJcj91Fep6AqMpLc7rPKdfgUwNmJ1H8A9QD8AmBAIvQbQGsAcwCchRNCIfD9VtqTh5pCwdW+B0191ArAjoj3+cq2IJAphCgAAOV/c2W7Wp9bKa+jt1f7jhCiHMABAE1ta7lJlKVuX1TNmgPff0WFshxAEYDZQoiE6DeA5wBMBlAZsS0R+g0AAsA3RLSUiK5Vtrnad0s1mj0IxdgWdJ9btT5rXQvPXyciagDgQwC3CSEOKirSmLvG2ObL/gshKgCcTESNAHxMRL00dg9Ev4noHABFQoilRDRUz1dibPNdvyMYJITYRUTNAcwmonUa+zrS96CtFPIBtIl43xrALpfaIptCIsoCAOV/kbJdrc/5yuvo7dW+Q0QpABoC2Gdbyw1CRLVQJRDeEkJ8pGxOmP4LIfYDmA9gNILf70EAziOiPADvADiLiN5E8PsNABBC7FL+FwH4GEB/uNz3oAmFJQA6E1F7IqqNKsPKTJfbJIuZACYpryehStce2n6Z4mXQHkBnAIuVZWcJEZ2meCJcGfWd0LEuAjBXKEpHt1HaOgPAWiHEMxEfBbr/RJShrBBARHUBjACwDgHvtxDibiFEayFENqqe17lCiIkIeL8BgIjqE1Fa6DWAkQBWwe2+u21oscFwMxZVHiubAdzjdntM9uFtAAUAylAl6a9GlR5wDoCNyv8mEfvfo/R3PRSvA2V7jnKTbQbwL5yIYK8D4H0Am1DltdDB7T5HtHkwqpa3KwEsV/7GBr3/AHoDWKb0exWA+5Xtge531DUYihOG5sD3G1VekiuUv9Wh8crtvnOaC4ZhGCZM0NRHDMMwjAVYKDAMwzBhWCgwDMMwYVgoMAzDMGFYKDAMwzBhWCgwDMMwYVgoMAzDMGH+H11ZLI/RZHjFAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    paths=marking[marking['fold'] != fold_number]['path'].values,\n    texts=marking[marking['fold'] != fold_number]['text_number'].values,\n    audio_augs=add_noise,\n)\n\nvalidation_dataset = DatasetRetriever(\n    paths=marking[marking['fold'] == fold_number]['path'].values,\n    texts=marking[marking['fold'] == fold_number]['text_number'].values,\n#    audio_augs=add_noise,\n)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_audio(batch):\n    waveforms = []\n    labels = []\n    texts = []\n    paths = []\n    bs = len(batch)\n    waveform_sizes = torch.empty(bs, dtype=torch.int)\n    label_sizes = torch.empty(bs, dtype=torch.int)\n    for i, sample in enumerate(batch):\n        waveforms.append(sample['waveform'])\n        labels.append(sample['labels'])\n        waveform_sizes[i] = sample['waveform'].shape[0]\n        label_sizes[i] = sample['labels'].shape[0]\n        texts.append(sample['text'])\n        paths.append(sample['path'])\n\n    return {\n        'waveforms': pad_sequence(waveforms, batch_first=True),\n        'labels': pad_sequence(labels, batch_first=True),\n        'waveform_sizes': waveform_sizes,\n        'label_sizes': label_sizes,\n        'texts': texts,\n        'paths': paths,\n    }","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    sampler=RandomSampler(train_dataset),\n    batch_size=16,\n    pin_memory=False,\n    drop_last=True,\n    num_workers=2,\n    collate_fn=collate_audio,\n)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in train_loader:\n    break\n    \nbatch['waveforms'].shape, batch['labels'].shape","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"(torch.Size([16, 57521]), torch.Size([16, 61]))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch['waveform_sizes'], batch['label_sizes']","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(tensor([36108, 57521, 43350, 49606, 51743, 48869, 38548, 44972, 48499, 49704,\n         50979, 50685, 57462, 55075, 41285, 53973], dtype=torch.int32),\n tensor([35, 61, 42, 52, 56, 50, 33, 47, 50, 51, 52, 50, 57, 56, 42, 53],\n        dtype=torch.int32))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch['texts']","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"['пятьдесят тысяч сто двадцать четыре',\n 'четыреста восемьдесят шесть тысяч шестьсот восемьдесят четыре',\n 'семьсот три тысячи пятьсот пятьдесят шесть',\n 'девятьсот восемьдесят тысяч девятьсот двадцать шесть',\n 'девятьсот семьдесят восемь тысяч девятьсот пятьдесят два',\n 'девятьсот сорок пять тысяч шестьсот девяносто пять',\n 'двести семнадцать тысяч сто сорок',\n 'восемьсот двадцать три тысячи пятьсот сорок три',\n 'шестьсот двадцать три тысячи двести девяносто один',\n 'пятьсот восемьдесят одна тысяча шестьсот сорок пять',\n 'пятьсот восемьдесят девять тысяч двести сорок восемь',\n 'семьсот восемьдесят две тысячи восемьсот пятьдесят',\n 'четыреста девяносто восемь тысяч восемьсот семьдесят семь',\n 'восемьсот пятьдесят четыре тысячи семьсот пятьдесят пять',\n 'шестьсот тридцать тысяч сто пятьдесят пять',\n 'семьсот восемнадцать тысяч четыреста пятьдесят девять']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Pretrained Jasper\n\n\nThe Jasper model is an end-to-end neural acoustic model for automatic speech recognition (ASR) that provides near state-of-the-art results on LibriSpeech among end-to-end ASR models without any external data. \n\nsource: https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper\n\n<img src=\"https://nvidia.github.io/NeMo/_images/quartz_vertical.png\" width=\"500\" align=\"left\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.insert(0, './DeepLearningExamples/PyTorch/SpeechRecognition/Jasper')\n\nimport toml\nfrom collections import OrderedDict\nfrom model import Jasper, JasperEncoderDecoder, JasperEncoder, AudioPreprocessing, CTCLossNM\n\n\nmodel_toml = '../input/pretrained-jasper10x5dr-nomask/jasper10x5dr_nomask.toml'\nmodel_definition = toml.load(model_toml)\n\nmodel_definition['labels']['labels'] = CHAR_LABELS.chars\nmodel_definition['encoder']['convmask'] = True\nmodel_definition['normalize_transcripts'] = False\nmodel_definition['input_eval']['normalize_transcripts'] = False\nmodel_definition['input']['normalize_transcripts'] = False\n\nfrom copy import deepcopy\n\nctc_vocab = deepcopy(model_definition['labels']['labels'])\nctc_vocab.append(\"<BLANK>\")\n\nfeaturizer_config = model_definition['input_eval']\nfeaturizer_config[\"optimization_level\"] = 3","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = CTCLossNM(num_classes=len(ctc_vocab))\nmodel = Jasper(feature_config=featurizer_config, jasper_model_definition=model_definition, feat_in=1024, num_classes=len(ctc_vocab))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(f'../input/pretrained-jasper10x5dr-nomask/jasper_fp16.pt')\npreprocessed_checkpoint = OrderedDict()\nfor key in checkpoint['state_dict'].keys():\n    if 'jasper_decoder' in key:\n        continue\n    preprocessed_checkpoint[key] = checkpoint['state_dict'][key]\n\nmodel.load_state_dict(preprocessed_checkpoint, strict=False)\nmodel = model.cuda()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"audio_preprocessor = AudioPreprocessing(**featurizer_config)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def freeze_until(net, param_name):\n    found_name = False\n    for name, params in net.named_parameters():\n        if name == param_name:\n            found_name = True\n        params.requires_grad = found_name\n\nfreeze_until(model, 'jasper_encoder.encoder.10.conv.0.weight')","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def levenshtein_distance(first, second):\n    \"\"\"\n    Compute Levenshtein distance between two array-like objects\n    \"\"\"\n    distance = [[0 for _ in range(len(second) + 1)] for _ in range(len(first) + 1)]\n    for i in range(len(first) + 1):\n        for j in range(len(second) + 1):\n            if i == 0:\n                distance[i][j] = j\n            elif j == 0:\n                distance[i][j] = i\n            else:\n                diag = distance[i - 1][j - 1] + (first[i - 1] != second[j - 1])\n                upper = distance[i - 1][j] + 1\n                left = distance[i][j - 1] + 1\n                distance[i][j] = min(diag, upper, left)\n    return distance[len(first)][len(second)]\n\ndef calculate_metrics(targets, outputs):\n    ser, wer = [], []\n    for output, target in zip(\n        outputs.argmax(axis=2).cpu().numpy(),\n        targets.cpu().numpy()\n    ):\n        target_text = CHAR_LABELS.get_text(target)\n        output_text = CHAR_LABELS.get_text(output)\n        ser.append(levenshtein_distance(\n            output_text, \n            target_text\n        ) / len(target_text))\n        wer.append(levenshtein_distance(\n            output_text.split(), \n            target_text.split(),\n        ) / len(target_text.split()))\n    return np.mean(ser), np.mean(wer)","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, audio_preprocessor, criterion, device, config):\n        self.config = config\n        self.epoch = 0\n        \n        self.base_dir = './'\n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.audio_preprocessor = audio_preprocessor\n        self.criterion = criterion\n        \n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, summary_ser, summary_wer = self.train_one_epoch(train_loader)\n            \n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            t = time.time()\n            summary_loss, summary_ser, summary_wer = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            self.model.eval()\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n            \n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.save(f'{self.base_dir}/best-checkpoint.bin')\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        self.audio_preprocessor.eval();\n        summary_loss = AverageMeter()\n        summary_wer = AverageMeter()\n        summary_ser = AverageMeter()\n        t = time.time()\n        for step, batch in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                t_audio_signal_t = batch['waveforms'].to(self.device)\n                t_a_sig_length_t = batch['waveform_sizes'].to(self.device)\n                t_transcript_t = batch['labels'].to(self.device)\n                t_transcript_len_t = batch['label_sizes'].to(self.device)\n                \n                t_processed_signal_t, t_processed_sig_length_t = self.audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n                t_log_probs_t, t_encoded_len_t = self.model.forward((t_processed_signal_t, t_processed_sig_length_t))\n\n                loss = self.criterion(log_probs=t_log_probs_t, targets=t_transcript_t, input_length=t_encoded_len_t, target_length=t_transcript_len_t)\n                \n                batch_size = t_a_sig_length_t.shape[0]\n                \n                ser, wer = calculate_metrics(t_transcript_t, t_log_probs_t)\n                summary_ser.update(ser, batch_size)\n                summary_wer.update(wer, batch_size)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss, summary_ser, summary_wer\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        self.audio_preprocessor.train();\n        summary_loss = AverageMeter()\n        summary_wer = AverageMeter()\n        summary_ser = AverageMeter()\n        t = time.time()\n        for step, batch in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n\n            t_audio_signal_t = batch['waveforms'].to(self.device)\n            t_a_sig_length_t = batch['waveform_sizes'].to(self.device)\n            t_transcript_t = batch['labels'].to(self.device)\n            t_transcript_len_t = batch['label_sizes'].to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            t_processed_signal_t, t_processed_sig_length_t = self.audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n            t_log_probs_t, t_encoded_len_t = self.model.forward((t_processed_signal_t, t_processed_sig_length_t))\n            \n            loss = self.criterion(log_probs=t_log_probs_t, targets=t_transcript_t, input_length=t_encoded_len_t, target_length=t_transcript_len_t)\n\n            batch_size = t_a_sig_length_t.shape[0]\n\n            loss.backward()\n\n            self.optimizer.step()\n            \n            with torch.no_grad():\n                ser, wer = calculate_metrics(t_transcript_t, t_log_probs_t)\n                summary_ser.update(ser, batch_size)\n                summary_wer.update(wer, batch_size)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss, summary_ser, summary_wer\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 16 \n    n_epochs = 10\n    lr = 0.001\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = True  # do scheduler.step after optimizer.step\n    validation_scheduler = False  # do scheduler.step after validation stage loss\n\n    SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n    scheduler_params = dict(\n        max_lr=0.001,\n        epochs=n_epochs,\n        steps_per_epoch=int(len(train_dataset) / batch_size),\n        pct_start=0.1,\n        anneal_strategy='cos', \n        final_div_factor=10**5\n    )\n\n#     SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n#     scheduler_params = dict(\n#         mode='min',\n#         factor=0.5,\n#         patience=1,\n#         verbose=False, \n#         threshold=0.0001,\n#         threshold_mode='abs',\n#         cooldown=0, \n#         min_lr=1e-8,\n#         eps=1e-08\n#     )\n    # --------------------","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training():\n    device = torch.device('cuda:0')\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=RandomSampler(train_dataset),\n        batch_size=TrainGlobalConfig.batch_size,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_audio,\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_audio,\n    )\n\n    fitter = Fitter(\n        model=model.to(device),\n        audio_preprocessor=audio_preprocessor.to(device),\n        criterion=criterion,\n        device=device, \n        config=TrainGlobalConfig,\n    )\n    fitter.fit(train_loader, val_loader)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training()","execution_count":27,"outputs":[{"output_type":"stream","text":"Fitter prepared. Device is cuda:0\n\n2020-07-24T09:48:28.920227\nLR: 3.9999999999999996e-05\n[RESULT]: Train. Epoch: 0, summary_loss: 52.02014, ser: 0.52204, wer: 1.28676, time: 188.46382\n[RESULT]: Val. Epoch: 0, summary_loss: 0.72616, ser: 0.00457, wer: 0.02920, time: 30.40261\n\n2020-07-24T09:52:21.108598\nLR: 0.0009999998495717124\n[RESULT]: Train. Epoch: 1, summary_loss: 0.51168, ser: 0.21904, wer: 0.80690, time: 184.91142\n[RESULT]: Val. Epoch: 1, summary_loss: 1.01855, ser: 0.00606, wer: 0.02513, time: 30.16324\n\n2020-07-24T09:56:02.607582\nLR: 0.0009697135283286831\n[RESULT]: Train. Epoch: 2, summary_loss: 0.44503, ser: 0.27059, wer: 0.84693, time: 184.57032\n[RESULT]: Val. Epoch: 2, summary_loss: 0.16205, ser: 0.02315, wer: 0.16877, time: 30.45623\n\n2020-07-24T10:00:11.451683\nLR: 0.0008827728474587917\n[RESULT]: Train. Epoch: 3, summary_loss: 0.03892, ser: 0.35879, wer: 1.02255, time: 186.13871\n[RESULT]: Val. Epoch: 3, summary_loss: 0.02823, ser: 0.02246, wer: 0.16559, time: 30.69170\n\n2020-07-24T10:04:20.905681\nLR: 0.0007496641361827421\n[RESULT]: Train. Epoch: 4, summary_loss: 0.02580, ser: 0.36470, wer: 0.71354, time: 186.41869\n[RESULT]: Val. Epoch: 4, summary_loss: 1.24384, ser: 0.02708, wer: 0.18636, time: 30.10209\n\n2020-07-24T10:08:04.474976\nLR: 0.0005864422695557817\n[RESULT]: Train. Epoch: 5, summary_loss: 0.02071, ser: 0.36555, wer: 0.53448, time: 185.85701\n[RESULT]: Val. Epoch: 5, summary_loss: 0.02290, ser: 0.02248, wer: 0.16596, time: 30.48175\n\n2020-07-24T10:12:15.106510\nLR: 0.0004127942135913184\n[RESULT]: Train. Epoch: 6, summary_loss: 0.01886, ser: 0.33196, wer: 0.48258, time: 185.87515\n[RESULT]: Val. Epoch: 6, summary_loss: 0.03267, ser: 0.02240, wer: 0.16583, time: 30.35900\n\n2020-07-24T10:15:58.411878\nLR: 0.00024966448661102966\n[RESULT]: Train. Epoch: 7, summary_loss: 0.00681, ser: 0.35542, wer: 0.63890, time: 184.86100\n[RESULT]: Val. Epoch: 7, summary_loss: 0.00847, ser: 0.02237, wer: 0.16568, time: 30.45367\n\n2020-07-24T10:20:07.808745\nLR: 0.0001167289412270987\n[RESULT]: Train. Epoch: 8, summary_loss: 0.00647, ser: 0.33668, wer: 0.63274, time: 185.46093\n[RESULT]: Val. Epoch: 8, summary_loss: 0.01403, ser: 0.02241, wer: 0.16587, time: 30.61519\n\n2020-07-24T10:23:50.611968\nLR: 3.0021566132526776e-05\n[RESULT]: Train. Epoch: 9, summary_loss: 0.00568, ser: 0.32815, wer: 0.59529, time: 186.00585\n[RESULT]: Val. Epoch: 9, summary_loss: 0.01104, ser: 0.02238, wer: 0.16579, time: 30.60265\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load(f'best-checkpoint.bin')['model_state_dict'])","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\ndef run_inference(model, audio_preprocessor, val_loader):\n    result = {\n        'reference': [],\n        'prediction': [],\n        'path': [],\n    }\n    model.eval();\n    audio_preprocessor.eval();\n    for batch in tqdm(val_loader, total=len(val_loader)):\n        with torch.no_grad():\n            t_audio_signal_t = batch['waveforms'].cuda()\n            t_a_sig_length_t = batch['waveform_sizes'].cuda()\n            result['reference'].extend(batch['texts'])\n            result['path'].extend(batch['paths'])\n\n            t_processed_signal_t, t_processed_sig_length_t = audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n            t_log_probs_t, t_encoded_len_t = model.forward((t_processed_signal_t, t_processed_sig_length_t))\n            \n            for output in t_log_probs_t.argmax(axis=2).cpu().numpy():\n                output_text = CHAR_LABELS.get_text(output)\n                result['prediction'].append(output_text)\n\n    return pd.DataFrame(result)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nval_loader = torch.utils.data.DataLoader(\n    validation_dataset, \n    batch_size=TrainGlobalConfig.batch_size,\n    num_workers=TrainGlobalConfig.num_workers,\n    shuffle=False,\n    sampler=SequentialSampler(validation_dataset),\n    pin_memory=False,\n    collate_fn=collate_audio,\n)\n\nresult = run_inference(model, audio_preprocessor, val_loader)","execution_count":30,"outputs":[{"output_type":"stream","text":"100%|██████████| 113/113 [00:24<00:00,  4.54it/s]","name":"stderr"},{"output_type":"stream","text":"CPU times: user 17.6 s, sys: 6.86 s, total: 24.4 s\nWall time: 24.9 s\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{}s per file to make prediction'.format(round(24.9 / 9000, 6)))","execution_count":42,"outputs":[{"output_type":"stream","text":"0.002767s per file to make prediction\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Result correction"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = set(word for string in marking.text_number for word in string.split())\n\ndef thousand(number: str) -> str:\n    if number == 'одна':\n        return 'тысяча'\n    elif number in ['две', 'три', 'четыре']:\n        return 'тысячи'\n    else:\n        return 'тысяч'\n\ndef correct(text: str) -> str:\n    words = list(text.split())\n    correct_words = []\n    for i, word in enumerate(words):\n        if word in corpus:\n            correct_words.append(word)        \n        else:\n            correction = ''\n            min_dist = len(word)\n            for variant in corpus:\n                if levenshtein_distance(word, variant) < min_dist:\n                    min_dist = levenshtein_distance(word, variant)\n                    correction = variant\n            correct_words.append(correction)\n                    \n        if correct_words[-1][:5] == 'тысяч' and i > 0:\n            correct_words[-1] = thousand(words[i - 1])      \n        \n    return ' '.join(correct_words)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult['prediction'] = result['prediction'].apply(correct)\nresult.head()","execution_count":32,"outputs":[{"output_type":"stream","text":"CPU times: user 5.6 s, sys: 4.62 ms, total: 5.61 s\nWall time: 5.6 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"                                       reference  \\\n0               четыре тысячи двести восемьдесят   \n1           восемьсот тысяч триста пятьдесят два   \n2       двадцать пять тысяч пятьсот тридцать два   \n3       девятьсот тридцать девять тысяч сто пять   \n4  сто тридцать девять тысяч двести девятнадцать   \n\n                                      prediction            path  \n0               четыре тысячи двести восемьдесят  5e39a8735b.wav  \n1           восемьсот тысяч триста пятьдесят два  d004a14da0.wav  \n2       двадцать пять тысяч пятьсот тридцать два  b90381298e.wav  \n3       девятьсот тридцать девять тысяч сто пять  b4c275ce77.wav  \n4  сто тридцать девять тысяч двести девятнадцать  0f3e842d88.wav  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>prediction</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>четыре тысячи двести восемьдесят</td>\n      <td>четыре тысячи двести восемьдесят</td>\n      <td>5e39a8735b.wav</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>восемьсот тысяч триста пятьдесят два</td>\n      <td>восемьсот тысяч триста пятьдесят два</td>\n      <td>d004a14da0.wav</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>двадцать пять тысяч пятьсот тридцать два</td>\n      <td>двадцать пять тысяч пятьсот тридцать два</td>\n      <td>b90381298e.wav</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>девятьсот тридцать девять тысяч сто пять</td>\n      <td>девятьсот тридцать девять тысяч сто пять</td>\n      <td>b4c275ce77.wav</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>сто тридцать девять тысяч двести девятнадцать</td>\n      <td>сто тридцать девять тысяч двести девятнадцать</td>\n      <td>0f3e842d88.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{}s per prediction to make spell correction'.format(round(5.6 / 9000, 6)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv(f'prediction-{fold_number}.csv', index=False)\nresult.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"                                       reference  \\\n0               четыре тысячи двести восемьдесят   \n1           восемьсот тысяч триста пятьдесят два   \n2       двадцать пять тысяч пятьсот тридцать два   \n3       девятьсот тридцать девять тысяч сто пять   \n4  сто тридцать девять тысяч двести девятнадцать   \n\n                                      prediction            path  \n0               четыре тысячи двести восемьдесят  5e39a8735b.wav  \n1           восемьсот тысяч триста пятьдесят два  d004a14da0.wav  \n2       двадцать пять тысяч пятьсот тридцать два  b90381298e.wav  \n3       девятьсот тридцать девять тысяч сто пять  b4c275ce77.wav  \n4  сто тридцать девять тысяч двести девятнадцать  0f3e842d88.wav  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>prediction</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>четыре тысячи двести восемьдесят</td>\n      <td>четыре тысячи двести восемьдесят</td>\n      <td>5e39a8735b.wav</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>восемьсот тысяч триста пятьдесят два</td>\n      <td>восемьсот тысяч триста пятьдесят два</td>\n      <td>d004a14da0.wav</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>двадцать пять тысяч пятьсот тридцать два</td>\n      <td>двадцать пять тысяч пятьсот тридцать два</td>\n      <td>b90381298e.wav</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>девятьсот тридцать девять тысяч сто пять</td>\n      <td>девятьсот тридцать девять тысяч сто пять</td>\n      <td>b4c275ce77.wav</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>сто тридцать девять тысяч двести девятнадцать</td>\n      <td>сто тридцать девять тысяч двести девятнадцать</td>\n      <td>0f3e842d88.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Fold Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"ser, wer = [], []\nfor prediction, reference in tqdm(zip(result['prediction'], result['reference']), total=result.shape[0]):\n    ser.append(levenshtein_distance(\n        prediction, \n        reference\n    ) / len(reference))\n    wer.append(levenshtein_distance(\n        prediction.split(), \n        reference.split(),\n    ) / len(reference.split()))","execution_count":34,"outputs":[{"output_type":"stream","text":"100%|██████████| 1800/1800 [00:05<00:00, 348.35it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-'*10 + f'fold-{fold_number}' + '-'*10)\nprint(f'[SER]: {np.mean(ser):.5f}')\nprint(f'[WER]: {np.mean(wer):.5f}')\nprint('-'*26)","execution_count":35,"outputs":[{"output_type":"stream","text":"----------fold-0----------\n[SER]: 0.00000\n[WER]: 0.00000\n--------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistakes = result[result['reference'] != result['prediction']]\nmistakes.shape","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"(0, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistakes['dist'] = mistakes.apply(lambda x: levenshtein_distance(x.reference, x.prediction), axis=1)\nmistakes.head(10)","execution_count":37,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Wrong number of items passed 3, placement implies 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'dist'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'dist'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-3973ee44bb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmistakes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmistakes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlevenshtein_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmistakes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3625\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   3045\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m   2593\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2595\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise ValueError(\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 3, placement implies 1"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython.display as ipd\nipd.Audio(f'{TRAIN_PATH}/{mistakes.iloc[-1][\"path\"]}' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct('четыре тысяч двести восмьдесят ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistakes['edited'] = mistakes['prediction'].apply(correct)\nmistakes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistakes['symbol_dist_after_correction'] = mistakes.apply(lambda x: levenshtein_distance(x.reference, x.edited), axis=1)\nmistakes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"left_mistakes = mistakes[mistakes['symbol_dist_after_correction'] != 0]\nleft_mistakes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank you for reading!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}